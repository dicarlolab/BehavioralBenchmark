{
 "metadata": {
  "name": "",
  "signature": "sha256:44232e959c2128e28902beffc5c9344031e602635f507dbc2b6d5022caf512f2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import mturkutils\n",
      "import os\n",
      "from os import listdir\n",
      "import sys\n",
      "sys.path = ['/mindhive/dicarlolab/u/toosi/tabular'] + sys.path\n",
      "import dldata\n",
      "from dldata.metrics.utils import compute_metric\n",
      "import dldata.stimulus_sets.hvm as hvm\n",
      "dataset = hvm.HvMWithDiscfade()\n",
      "dataset = dataset\n",
      "meta = dataset .meta\n",
      "import dldata.physiology.hongmajaj.mappings as m\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Can't import separate mcc package\n",
        "common attributes, forcing a renaming ..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Using default renamer ...\n",
        "('Replacing columns', ['rxz', 'rxy', 'ryz'])"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import dldata.human_data.confusion_matrices as cm\n",
      "import matplotlib.cm as cmplot\n",
      "\n",
      "import dldata.metrics.utils\n",
      "import dldata.metrics.classifier\n",
      "reload(dldata.metrics.classifier)\n",
      "reload(dldata.metrics.utils)\n",
      "reload(dldata.human_data.confusion_matrices)\n",
      "import cPickle\n",
      "from dldata.metrics import utils\n",
      "\n",
      "import pymongo as pm\n",
      "import scipy as sp\n",
      "import scipy.stats as ss\n",
      "from numpy import mean\n",
      "import numpy as np\n",
      "import pylab as pl\n",
      "import tables as tbl\n",
      "import os\n",
      "import cPickle as pk\n",
      "from operator import truediv\n",
      "\n",
      "import numpy as np\n",
      "import pymongo as pm\n",
      "import tabular as tb\n",
      "import random as rnd\n",
      "import time\n",
      "date= time.strftime(\"%d_%m_%Y\")\n",
      "#Comparings two datasets at the same time\n",
      "\n",
      "\n",
      "dprime_metric = dldata.metrics.utils.dprime\n",
      "compute_metric = dldata.metrics.utils.compute_metric\n",
      "compute_metric_base = dldata.metrics.utils.compute_metric_base\n",
      "get_confusion_matrix =  dldata.metrics.utils.classifier.get_confusion_matrix\n",
      "get_dprime =  dldata.metrics.utils.dprime\n",
      "get_dprime_bangmetric =  dldata.metrics.utils.dprime_bangmetric\n",
      "#get_data = dldata.human_data.confusion_matrices\n",
      "import dldata.metrics.utils\n",
      "reload(dldata.metrics.utils)\n",
      "compute_metric = dldata.metrics.utils.compute_metric\n",
      "imgs2show = dataset.get_images(preproc={'resize_to': (256,256), 'dtype': 'float32', 'mode':'L', 'normalize': False})\n",
      "IT_Chabo_LST =dldata.stimulus_sets.hvm.mappings.LST_IT_Chabo\n",
      "IT_Tito_LST =dldata.stimulus_sets.hvm.mappings.LST_IT_Tito\n",
      "V4_Chabo_LST =dldata.stimulus_sets.hvm.mappings.LST_V4_Chabo\n",
      "V4_Tito_LST =dldata.stimulus_sets.hvm.mappings.LST_V4_Tito\n",
      "\n",
      "V4_Chabo = dataset.neuronal_features[:, V4_Chabo_LST]\n",
      "V4_Tito = dataset.neuronal_features[:, V4_Tito_LST]\n",
      "IT_Chabo = dataset.neuronal_features[:, IT_Chabo_LST]\n",
      "IT_Tito = dataset.neuronal_features[:, IT_Tito_LST]\n",
      "Lst_Neurons_H = m.LST_IT_Chabo  + m.LST_IT_Tito \n",
      "print dataset.neuronal_features[:, Lst_Neurons_H].shape\n",
      "#IT = dataset.neuronal_features[:, dataset.LST_IT_Chabo]\n",
      "#IT = IT_Tito #IT_Tito  IT_Chabo V4_Chabo\n",
      "IT = np.concatenate((IT_Tito, IT_Chabo), axis = 1) \n",
      "n_cells = shape(IT)[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Can't import separate mcc package\n",
        "(5760, 168)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/site-packages/tables/leaf.py:385: PerformanceWarning: The Leaf ``/spk`` is exceeding the maximum recommended rowsize (104857600 bytes);\n",
        "be ready to see PyTables asking for *lots* of memory and possibly slow\n",
        "I/O.  You may want to reduce the rowsize by trimming the value of\n",
        "dimensions that are orthogonal (and preferably close) to the *main*\n",
        "dimension of this leave.  Alternatively, in case you have specified a\n",
        "very small/large chunksize, you may want to increase/decrease it.\n",
        "  PerformanceWarning)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import dldata.human_data.confusion_matrices as CM\n",
      "import tabular as tb\n",
      "coll = pm.MongoClient(port=22334)['mturk']['hvm_dense_smp_v6_2rpw']\n",
      "CM.update_collection_stimduration(coll)\n",
      "raw_data = CM.get_data('hvm_dense_smp_v6_2rpw', field='category')\n",
      "#Add rep number to raw data, then clean\n",
      "which_rep = {}\n",
      "for worker in np.unique(raw_data['WorkerId']):\n",
      "    which_rep[worker] = {}\n",
      "    for filename in np.unique(raw_data['filename']):\n",
      "        which_rep[worker][filename] = 0\n",
      "rep = np.zeros(raw_data['filename'].shape[0])\n",
      "for i, trial in enumerate(raw_data):\n",
      "    filename = trial['filename']\n",
      "    worker = trial['WorkerId']\n",
      "    rep[i] = which_rep[worker][filename]\n",
      "    which_rep[worker][filename] = which_rep[worker][filename]+1\n",
      "raw_data_with_rep = raw_data.addcols([rep], names=['rep'])\n",
      "\n",
      "#Get rid of everything but first two trials, get rid of learning reps (Images of V3 and V0)\n",
      "data = raw_data_with_rep[raw_data_with_rep['rep']<2]\n",
      "data = data[data['var'] == 'V6']\n",
      "\n",
      "canonical_order = np.unique(data['filename'])\n",
      "\n",
      "filename_to_id = {}\n",
      "for trial in data:\n",
      "    filename_to_id[trial['filename']] = trial['_id']\n",
      "Images = [filename_to_id[filename] for filename in canonical_order]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## 0 - choose the timebins and find the corresponding successive idx s.\n",
      "## 1 - Build an array of reps*images*units for each of V0 V3 and V6 by summing over desired timebins\n",
      "## 2-0 - Bootstrap each array by selecting a rep over image/site pairs\n",
      "## 2- 1- Concatenate v0,v3,v6\n",
      "\n",
      "\n",
      "## 0 - choose the timebins and find the corresponding successive idx s.\n",
      "#*************************************************************************\n",
      "os.chdir('/mindhive/dicarlolab/u/hahong/tmp')\n",
      "#dummy: just to obtain time windows\n",
      "dxt = tbl.openFile('Chabo_Tito_20140307_Var00a_pooled_P58.trim.raw.repr.h5')\n",
      "TimeBins = dxt.root.meta.idx2lbl.read()\n",
      "TimeBinCenters = np.zeros((len(TimeBins)))\n",
      "Times = TimeBins\n",
      "for t in range(len(Times)):\n",
      "    TimeBinCenters[t] = mean(TimeBins[t])\n",
      "del dxt\n",
      "#====================================================================\n",
      "Selected_TimeBins = [[80,100]]\n",
      "#[[60,80],[80,100],[100,120],[120,140],[140,160],[160,180]]\n",
      "                     # ,[180,200],[200,220],[220,240],[240,260]]\n",
      "def neural_features_bin(Selected_TimeBins):\n",
      "    os.chdir('/mindhive/dicarlolab/u/hahong/tmp')\n",
      "\n",
      "\n",
      "    IDXs =  [i for i,v in enumerate(TimeBins) if v in Selected_TimeBins]\n",
      "    #print IDXs\n",
      "    ## 1 - Build an array of reps*images*units for each of V0 V3 and V6 by summing over desired timebins\n",
      "\n",
      "    dVar = dict()\n",
      "    for varlev in ['V0', 'V3', 'V6']:\n",
      "        if varlev == 'V0':\n",
      "            xt = tbl.openFile('Chabo_Tito_20140307_Var00a_pooled_P58.trim.raw.repr.h5')\n",
      "        elif  varlev == 'V3':\n",
      "            xt = tbl.openFile('Chabo_Tito_20140307_Var03a_pooled_P58.trim.raw.repr.h5')\n",
      "        elif varlev == 'V6':\n",
      "            xt = tbl.openFile('Chabo_Tito_20140307_Var06a_pooled_P58.trim.raw.repr.h5')\n",
      "    \n",
      "    \n",
      "        sumVar = 0\n",
      "        for idx in IDXs:\n",
      "            #print TimeBinCenters[idx]\n",
      "\n",
      "            \n",
      "            filenames = [os.path.basename(e) for e in dataset.meta[dataset.meta['var'] == varlev]['filename']]\n",
      "            idx2iidt = xt.root.meta.idx2iid.read()\n",
      "            lst_idx_vt = [idx2iidt.index(e) for e in filenames]\n",
      "            \n",
      "            Mbint0_reps = xt.root.spk[idx,:,:] # All reps\n",
      "            Mbint1_reps = Mbint0_reps[:,lst_idx_vt, :]# All reps\n",
      "            Mbint1_reps_LstN = Mbint1_reps[:,:,Lst_Neurons_H]\n",
      "            sumVar = sumVar + Mbint1_reps_LstN\n",
      "        if varlev == 'V0':\n",
      "            dVar['V0'] = sumVar\n",
      "        elif  varlev == 'V3':\n",
      "            dVar['V3'] = sumVar\n",
      "        elif varlev == 'V6':\n",
      "            dVar['V6'] = sumVar\n",
      "            \n",
      "    dVar['V0']= dVar['V0']/(float(len(IDXs))*0.02)\n",
      "    dVar['V3']= dVar['V3']/(float(len(IDXs))*0.02)\n",
      "    dVar['V6']= dVar['V6']/(float(len(IDXs))*0.02)\n",
      "    \n",
      "    dVarTimeRes = np.concatenate((dVar['V0'].mean(0),dVar['V3'].mean(0),dVar['V6'].mean(0)))\n",
      "    return dVarTimeRes\n",
      "\n",
      "#neural_features_bin(TimeBins[15:20])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DataInfo ={}\n",
      "DataInfo['Name'] = 'HvM'\n",
      "DataInfo['feature'] = 'IT'\n",
      "DataInfo['metaOrig'] = meta\n",
      "DataInfo['meta'] = meta\n",
      "DataInfo['var'] = 'v6'\n",
      "DataInfo['Array'] = dataset.neuronal_features[:,dataset.IT_NEURONS]\n",
      "\n",
      "DataInfo['TimeBin'] = [Selected_TimeBins[0][0],Selected_TimeBins[-1][-1]]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Decoder Model :Standard Sampled Neurons : with replacement"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from copy import deepcopy\n",
      "import time\n",
      "date= time.strftime(\"%d_%m_%Y\")\n",
      "#Comparings two datasets at the same time\n",
      "import dldata.metrics.utils\n",
      "import dldata.metrics.classifier\n",
      "reload(dldata.metrics.classifier)\n",
      "reload(dldata.metrics.utils)\n",
      "dprime_metric = dldata.metrics.utils.dprime\n",
      "compute_metric = dldata.metrics.utils.compute_metric\n",
      "compute_metric_base = dldata.metrics.utils.compute_metric_base\n",
      "get_confusion_matrix =  dldata.metrics.utils.classifier.get_confusion_matrix\n",
      "get_dprime =  dldata.metrics.utils.dprime\n",
      "get_dprime_bangmetric =  dldata.metrics.utils.dprime_bangmetric\n",
      "\n",
      "\n",
      "Names = ['Row','Im_id','Category','iCategory','Var']\n",
      "Names.extend(['Classifier','Features','n_test','n_train','n_splits','TimeStart','TimeEnd'])\n",
      "Names.extend(['nboot','nsamp','split'])\n",
      "Names.extend(['PrCat','iPrCat'])\n",
      "\n",
      "n_boots = 100\n",
      "ImageSetTest = Images\n",
      "NeuralSamples = [2,8,32,64,128,168]\n",
      "labelfunc = 'category'\n",
      "n_train = None\n",
      "n_test = len(ImageSetTest)\n",
      "n_splits = 1\n",
      "Classifier = 'svm.LinearSVC'#'svm.LinearSVC'\n",
      "eval_config     = { \"train_q\": lambda x : ((x['var'] in 'V6' )and (x['_id'] not in ImageSetTest)),\n",
      "                    \"test_q\": lambda x : (x['_id'] in ImageSetTest) ,\n",
      "                    \"labelfunc\": labelfunc,\n",
      "                    \"split_by\": None,\n",
      "                    \"npc_train\": n_train,\n",
      "                    \"npc_test\": n_test,\n",
      "                    \"npc_validate\": 0,\n",
      "                    \"num_splits\": n_splits,\n",
      "                    \"metric_screen\": \"classifier\",\n",
      "                    \"metric_kwargs\": {'model_type': Classifier ,\n",
      "                                      'model_kwargs': {'GridSearchCV_params':{'C': \n",
      "                                    [1e-5, 1e-4, 1e-3,.25e-3, .5e-3, .75e-3, 1e-2, .25e-2, .5e-2, .75e-2,  1e-1, 1, 10]}}\n",
      "                                                 }\n",
      "                                }\n",
      "\n",
      "#'model_kwargs': {'GridSearchCV_params':{'C': [1e-5, 1e-4, 1e-3, \n",
      "#                                  .25e-3, .5e-3, .75e-3, 1e-2, .25e-2, .5e-2, .75e-2,  1e-1, 1, 10]}}\n",
      "rng = np.random.RandomState(0)\n",
      "for in_s in range(len(NeuralSamples)):\n",
      "    print in_s\n",
      "    Recs = []\n",
      "    rspnb = 0\n",
      "    for nb in range(n_boots):\n",
      "        Data = deepcopy(DataInfo['Array'])\n",
      "        samps = rng.choice(Data.shape[1],NeuralSamples[in_s])\n",
      "        Data =  Data[:,samps]\n",
      "\n",
      "        meta = meta\n",
      "        result = compute_metric_base(Data, meta, eval_config, attach_models=True, return_splits=True)\n",
      "        #print 1- result['multiacc_loss']\n",
      "\n",
      "\n",
      "\n",
      "        Categories = unique(meta['category'])\n",
      "        testCorrSum = np.zeros((len(Images)))\n",
      "        testpresend = np.zeros((len(Images)))\n",
      "        iPCats = np.zeros((len(Images),8))\n",
      "\n",
      "        for sp in range(len(result['splits'][0])):\n",
      "            for testIm in result['splits'][0][sp]['test']:\n",
      "                testImid = meta[testIm]['_id']\n",
      "                Category = meta[testIm]['category']\n",
      "                var = meta[testIm]['var']\n",
      "                iCategory = [i for i,v in enumerate(Categories) if v== Category][0]\n",
      "                indXimtest = [i for i,v in enumerate(Images) if v == testImid][0]\n",
      "                indimtest = [i for i,v in enumerate(result['splits'][0][sp]['test']) if v == testIm][0]\n",
      "\n",
      "                testCorrSum[indXimtest] = testCorrSum[indXimtest] +  \\\n",
      "                                              sum(not(result['split_results'][sp]['test_errors'][0][indimtest]))\n",
      "                testpresend[indXimtest] = testpresend[indXimtest] + 1\n",
      "                rCat = result['split_results'][sp]['test_prediction'][indimtest]\n",
      "                irCat = [i for i,v in enumerate(Categories) if v==rCat][0]\n",
      "                iPCats[indXimtest,irCat] = iPCats[indXimtest,irCat] + 1 \n",
      "                rec = (rspnb,testImid,Category,iCategory,var)\n",
      "                rec += (Classifier,DataInfo['feature'],)\n",
      "                rec += (n_test,n_train,n_splits,DataInfo['TimeBin'][0],DataInfo['TimeBin'][1],)\n",
      "                rec += (nb,NeuralSamples[in_s],sp)\n",
      "                rec += (rCat,irCat,)\n",
      "                rspnb +=1\n",
      "                Recs.append(rec)\n",
      "\n",
      "            \n",
      "    XCSamp = tb.tabarray(records = Recs, names=Names)\n",
      "\n",
      "    os.chdir(\"/mindhive/dicarlolab/u/toosi/projs/Consistency/\")\n",
      "    import time\n",
      "    date= time.strftime(\"%d_%m_%Y\")    \n",
      "    if  (os.path.isdir(\"%s\"%((date))))== False:\n",
      "        os.mkdir(\"%s\"%((date)))\n",
      "    os.chdir(\"%s\"%((date)))\n",
      "    filenameMode = 'XCSampstandard_v6_128Images%i'%((NeuralSamples[in_s]))+'.csv'\n",
      "    XCSamp.saveSV(filenameMode)\n",
      "    \n",
      "    import cPickle\n",
      "    cPickle.dump(XCSamp, open(filenameMode[:-4], 'wb'))\n",
      "\n",
      "    \n",
      "    os.chdir(\"/mindhive/dicarlolab/u/toosi/projs/Consistency/\")\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Can't import separate mcc package\n",
        "0\n",
        "1"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}